
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
   <meta http-equiv="content-type" content="text/html; charset=utf-8" />
   <title>Usage of TF IDF &larr; </title>
   <meta name="author" content="Yisha Wu" />

   <link rel="start" href="/" />

	
	
	
	

   <!-- syntax highlighting CSS -->
   <link rel="stylesheet" href="/assets/themes/mark-reid/css/syntax.css" type="text/css" />

   <!-- Homepage CSS -->
   <link rel="stylesheet" href="/assets/themes/mark-reid/css/screen.css" type="text/css" />

</head>
<body id="">
<div id="site">
  
  <div id="header">
    <h1>
    	<a href="/" title="StarCroce">StarCroce</a>
    	<span class="byline">&larr; <a href="/">Yisha Wu</a></span>
    </h1>
    <ul class="nav">
      <li><a class="home" href="/">Home</a></li>
      <li><a  href="/archive.html">Archive</a></li>
      <li><a  href="/pages.html">Pages</a></li>
      <li><a  href="/categories.html">Categories</a></li>
      <li><a  href="/tags.html">Tags</a></li>
    </ul>
  </div>

  
<div id="page">
	
  <h1 class="emphnext">Usage of TF IDF</h1>
  <ul class="tag_box inline">
  
  


  
     
    	<li><a href="/tags.html#NLP-ref">NLP <span>3</span></a></li>
     
    	<li><a href="/tags.html#Text Classification-ref">Text Classification <span>3</span></a></li>
     
    	<li><a href="/tags.html#Data Mining-ref">Data Mining <span>3</span></a></li>
     
    	<li><a href="/tags.html#Tf-idf-ref">Tf-idf <span>1</span></a></li>
     
    	<li><a href="/tags.html#LDA-ref">LDA <span>1</span></a></li>
    
  



  </ul>

  
<h3 id="defination">Defination</h3>
<p>TF - IDF is short for <strong>term frequency - inverse document frequency</strong>. The wiki page can be found here: <a href="http://en.wikipedia.org/wiki/Tf–idf">tf - idf</a>. </p>

<p>Term frequency means the raw frequency of a term <em>t</em> in occurs in a document <em>d</em>, which can be represented as <em>tf(f, d) = f(t, d)</em>. </p>

<p>Inverse document frequency measures whether the term is common or rare across all documents. It is the logarithmically scaled fraction of the documents that contain the word, obtained by <strong>dividing the total number of documents by the number of documents containing the term</strong>, and then taking the logarithm of that quotient, which can be represented as <em>idf(t, D) = log(N / {d in D: t in d})</em>.</p>

<p>Finally <em>tfidf(t, d, D) = tf(t, d) x idf(t, D)</em>.</p>

<h3 id="usage">Usage</h3>
<p>Apparently, if a term <em>t</em> has a very high <em>tf</em> in one document, it means that this term appears lots of times in that document. Then we can believe that this term can help us classify this document.</p>

<p>If the number of documents containing term <em>t</em> is very small, the <em>idf</em> will be larger. Then we can also believe that the term <em>t</em> can help us classify this kind of documents. And idf can help us find some common words like “a”, “the”, “is”. These words can be appeared many times in a document, so the tf value will be very large. But they are also existed in every document, so the idf value will be very small, even 0. Then the tf-idf can help us filter these words.</p>

<h3 id="disadvantage">Disadvantage</h3>
<p>A basic assumption of tf-idf is that the most meaningful word that help us classify the document should appear most times. However, it is not always correct. A better way is to introduce semantic analysis. A recent way is to use LDA (Latent Dirichlet Allocation). I haven’t use this in my experiment, but I will try to talk a little about that in the next section.</p>

<h3 id="lda">LDA</h3>
<p>LDA is short for Latent Dirichlet Allocation. Here is a very good explanation about LDA on Quora if you don’t want to read the boring wiki page:</p>

<p><a href="http://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation">http://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation</a></p>

<p>Basically, LDA is an unsupervised learning algorithm. It can provide the topics that each document contains. If given 3 documents and 2 topics, LDA can return something like “document A is 100% topic 1, document B is 100% topic 2, document C is 80% topic 1 and 20% topic 2”.</p>

<p>LDA assumes that documents are mixtures of topics. In this model, LDA assumes that documents are created in the following steps:</p>

<ul>
  <li>Decide how many words that a document have.</li>
  <li>Choose a topic mixture for the document. For example, we have two topics “food” and “animal”, and we decide to create a document with 30% “food” and 70% “animal”.</li>
  <li>Pick a topic. So we have 30% probability to pick “food” and 70% probability to pick “animal”.</li>
  <li>For each topic we picked, generate a word. For example, “food” may generate “beef” with 40% and “animal” will generate “dog” with 35%.</li>
</ul>

<p>After that, LDA will try to backtrack from the document to find a set of topics that is most likely to generate this document.</p>

<p>This is just a simple example about how LDA works. I will explain it after I try it in my experiment and read more documents.</p>


  <address class="signature">
    <a class="author" href="/">Yisha Wu</a> 
    <span class="date">08 October 2014</span>
    <span class="location"></span>
  </address>
  
  <div class="prev-next">
  
    <a href="/tech/2014/10/09/python-encoding-in-my-project" class="next" title="Python Encoding in My Project">Next Post &rarr;</a>
  
  
    <a href="/tech/2014/10/07/text-sentiment-analysis" class="prev" title="Text Sentiment Analysis">&larr; Earlier Post</a>
  
  </div>
  
</div><!-- End Page -->




  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'starcrocegithubio'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>






  
  <div id="footer">
  	<address>
  		<span class="copyright">
  			Content by <a href="/info/site.html">Yisha Wu</a>. Design by 
  			<a href="http://mark.reid.name/">Mark Reid</a>
  			<br/>
  			(<a rel="licence" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Some rights reserved</a>)			
  		</span>
  		<span class="engine">
  			Powered by <a href="http://github.com/mojombo/jekyll/" title="A static, minimalist CMS">Jekyll</a>
  		</span>
  	</address>
  </div>
  
</div>

<!--[if IE 6]>
<script type="text/javascript"> 
	/*Load jQuery if not already loaded*/ if(typeof jQuery == 'undefined'){ document.write("<script type=\"text/javascript\"   src=\"http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js\"></"+"script>"); var __noconflict = true; } 
	var IE6UPDATE_OPTIONS = {
		icons_path: "http://static.ie6update.com/hosted/ie6update/images/"
	}
</script>
<script type="text/javascript" src="http://static.ie6update.com/hosted/ie6update/ie6update.js"></script>
<![endif]-->

  
</body>
</html>

